<html>

<head>
<style type="text/css">
.inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
}
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.left {
  text-align: left;
}
.right {
  text-align: right;
}
.center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}
</style>
<title>8.3 Labs: Decision Trees</title>
</head>

<body>

<h1>Decision Trees - Fitting Classification Trees</h1>

<div class="chunk" id="unnamed-chunk-1"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">rm</span><span class="hl std">(</span><span class="hl kwc">list</span><span class="hl std">=</span><span class="hl kwd">ls</span><span class="hl std">())</span>
<span class="hl com"># Load libraries function</span>
<span class="hl std">installIfAbsentAndLoad</span> <span class="hl kwb">&lt;-</span> <span class="hl kwa">function</span><span class="hl std">(</span><span class="hl kwc">neededVector</span><span class="hl std">) {</span>
  <span class="hl kwa">for</span><span class="hl std">(thispackage</span> <span class="hl kwa">in</span> <span class="hl std">neededVector) {</span>
    <span class="hl kwa">if</span><span class="hl std">(</span> <span class="hl opt">!</span> <span class="hl kwd">require</span><span class="hl std">(thispackage,</span> <span class="hl kwc">character.only</span> <span class="hl std">= T) )</span>
    <span class="hl std">{</span> <span class="hl kwd">install.packages</span><span class="hl std">(thispackage)}</span>
    <span class="hl kwd">library</span><span class="hl std">(thispackage,</span> <span class="hl kwc">character.only</span> <span class="hl std">= T)</span>
  <span class="hl std">}</span>
<span class="hl std">}</span>
<span class="hl kwd">par</span><span class="hl std">(</span><span class="hl kwc">mfrow</span> <span class="hl std">=</span><span class="hl kwd">c</span><span class="hl std">(</span><span class="hl num">1</span><span class="hl std">,</span><span class="hl num">1</span><span class="hl std">))</span>
</pre></div>
</div></div>

<p>We first use classification trees to analyze the Carseats data set. In these data, Sales is a continuous variable,
and so we begin by recoding it as a binary variable. We use the ifelse() function to create a variable, called High,
which takes on a value of Yes if the Sales variable exceeds 8, and takes on a value of No otherwise.
The tree library is used to construct classification and regression trees:</p>
<div class="chunk" id="unnamed-chunk-2"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">needed</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">c</span><span class="hl std">(</span><span class="hl str">'tree'</span><span class="hl std">,</span> <span class="hl str">&quot;ISLR&quot;</span><span class="hl std">)</span>
<span class="hl kwd">library</span><span class="hl std">(ISLR)</span>
<span class="hl kwd">library</span><span class="hl std">(tree)</span>
</pre></div>
<div class="warning"><pre class="knitr r">## Warning: package 'tree' was built under R version 3.6.2
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl kwd">attach</span><span class="hl std">(Carseats)</span>
<span class="hl std">High</span> <span class="hl kwb">=</span> <span class="hl kwd">ifelse</span><span class="hl std">(Sales</span> <span class="hl opt">&lt;=</span><span class="hl num">8</span> <span class="hl std">,</span> <span class="hl str">&quot;No&quot;</span><span class="hl std">,</span> <span class="hl str">&quot;Yes&quot;</span><span class="hl std">)</span>
</pre></div>
</div></div>

<p>Then use the data.frame() function to merge High with the rest of the Carseats data </p>
<div class="chunk" id="unnamed-chunk-3"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">Carseats</span> <span class="hl kwb">=</span> <span class="hl kwd">data.frame</span><span class="hl std">(Carseats, High)</span>
</pre></div>
</div></div>

<p>We now use the tree() function to fit a classification tree in order to predict High using all variables but Sales.
The syntax of the tree() function is quite similar to that of the lm() function </p>
<div class="chunk" id="unnamed-chunk-4"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">tree.carseats</span> <span class="hl kwb">=</span> <span class="hl kwd">tree</span><span class="hl std">(High</span> <span class="hl opt">~</span> <span class="hl std">.</span> <span class="hl opt">-</span><span class="hl std">Sales , Carseats)</span>
</pre></div>
</div></div>

<p>#The summary() function lists the variables that are used as internal nodes in the tree, the number of terminal nodes,
# and the training error rate. </p>
<div class="chunk" id="unnamed-chunk-5"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">summary</span><span class="hl std">(tree.carseats)</span>
</pre></div>
<div class="output"><pre class="knitr r">## 
## Classification tree:
## tree(formula = High ~ . - Sales, data = Carseats)
## Variables actually used in tree construction:
## [1] &quot;ShelveLoc&quot;   &quot;Price&quot;       &quot;Income&quot;      &quot;CompPrice&quot;   &quot;Population&quot; 
## [6] &quot;Advertising&quot; &quot;Age&quot;         &quot;US&quot;         
## Number of terminal nodes:  27 
## Residual mean deviance:  0.4575 = 170.7 / 373 
## Misclassification error rate: 0.09 = 36 / 400
</pre></div>
</div></div>

<p>#We see that the training error rate is 9%. For classification trees, the deviance reported in the output of summary()
# is given by:
# [insert image of the sumon page 325 if possible]
#where n(mk) is the number of observations in the mth terminal node that belongs to the kth class. A small deviance
# indicates a tree that provides a good fit to the training data. The residual mean deviance reported is simply the 
# deviance divided by n- |T0|, which in this case is 400 - 27 = 373.
# One of the most attractive properties of trees is that they can be graphicallly displayed. We use the plot() function
# to display the tree strucutre, and the text() function to display the node labels. The argument pretty = 0 instructs
# R to include sthe category names for any qualititaitve predictors, rather than simply displaying a letter for each
# category  </p>

<div class="chunk" id="unnamed-chunk-6"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">plot</span><span class="hl std">(tree.carseats)</span>
<span class="hl kwd">text</span><span class="hl std">(tree.carseats,</span><span class="hl kwc">pretty</span><span class="hl std">=</span><span class="hl num">0</span><span class="hl std">)</span>
</pre></div>
</div><div class="rimage default"><img src="figure/unnamed-chunk-6-1.png" title="plot of chunk unnamed-chunk-6" alt="plot of chunk unnamed-chunk-6" class="plot" /></div></div>

<p>#The most important indicator of Sales appears to be shelving location, since the first branch differentiates Good
# locatiosn from Bad and Medium locations.
# If we just type the name of the tree object, R prints output corresponsing to each branch of the tree. R displays
# the split criterion (e.g. Price <92.5), the number of observations in that branch, the deviance, the overall 
# prediction for the branch (Yes or No), and the fraction of observations in that branch that take on values 
# of Yes and No. Branches that lead to terminal nodes are indicates using asterisks. </p>

<div class="chunk" id="unnamed-chunk-7"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">tree.carseats</span>
</pre></div>
<div class="output"><pre class="knitr r">## node), split, n, deviance, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 400 541.500 No ( 0.59000 0.41000 )  
##     2) ShelveLoc: Bad,Medium 315 390.600 No ( 0.68889 0.31111 )  
##       4) Price &lt; 92.5 46  56.530 Yes ( 0.30435 0.69565 )  
##         8) Income &lt; 57 10  12.220 No ( 0.70000 0.30000 )  
##          16) CompPrice &lt; 110.5 5   0.000 No ( 1.00000 0.00000 ) *
##          17) CompPrice &gt; 110.5 5   6.730 Yes ( 0.40000 0.60000 ) *
##         9) Income &gt; 57 36  35.470 Yes ( 0.19444 0.80556 )  
##          18) Population &lt; 207.5 16  21.170 Yes ( 0.37500 0.62500 ) *
##          19) Population &gt; 207.5 20   7.941 Yes ( 0.05000 0.95000 ) *
##       5) Price &gt; 92.5 269 299.800 No ( 0.75465 0.24535 )  
##        10) Advertising &lt; 13.5 224 213.200 No ( 0.81696 0.18304 )  
##          20) CompPrice &lt; 124.5 96  44.890 No ( 0.93750 0.06250 )  
##            40) Price &lt; 106.5 38  33.150 No ( 0.84211 0.15789 )  
##              80) Population &lt; 177 12  16.300 No ( 0.58333 0.41667 )  
##               160) Income &lt; 60.5 6   0.000 No ( 1.00000 0.00000 ) *
##               161) Income &gt; 60.5 6   5.407 Yes ( 0.16667 0.83333 ) *
##              81) Population &gt; 177 26   8.477 No ( 0.96154 0.03846 ) *
##            41) Price &gt; 106.5 58   0.000 No ( 1.00000 0.00000 ) *
##          21) CompPrice &gt; 124.5 128 150.200 No ( 0.72656 0.27344 )  
##            42) Price &lt; 122.5 51  70.680 Yes ( 0.49020 0.50980 )  
##              84) ShelveLoc: Bad 11   6.702 No ( 0.90909 0.09091 ) *
##              85) ShelveLoc: Medium 40  52.930 Yes ( 0.37500 0.62500 )  
##               170) Price &lt; 109.5 16   7.481 Yes ( 0.06250 0.93750 ) *
##               171) Price &gt; 109.5 24  32.600 No ( 0.58333 0.41667 )  
##                 342) Age &lt; 49.5 13  16.050 Yes ( 0.30769 0.69231 ) *
##                 343) Age &gt; 49.5 11   6.702 No ( 0.90909 0.09091 ) *
##            43) Price &gt; 122.5 77  55.540 No ( 0.88312 0.11688 )  
##              86) CompPrice &lt; 147.5 58  17.400 No ( 0.96552 0.03448 ) *
##              87) CompPrice &gt; 147.5 19  25.010 No ( 0.63158 0.36842 )  
##               174) Price &lt; 147 12  16.300 Yes ( 0.41667 0.58333 )  
##                 348) CompPrice &lt; 152.5 7   5.742 Yes ( 0.14286 0.85714 ) *
##                 349) CompPrice &gt; 152.5 5   5.004 No ( 0.80000 0.20000 ) *
##               175) Price &gt; 147 7   0.000 No ( 1.00000 0.00000 ) *
##        11) Advertising &gt; 13.5 45  61.830 Yes ( 0.44444 0.55556 )  
##          22) Age &lt; 54.5 25  25.020 Yes ( 0.20000 0.80000 )  
##            44) CompPrice &lt; 130.5 14  18.250 Yes ( 0.35714 0.64286 )  
##              88) Income &lt; 100 9  12.370 No ( 0.55556 0.44444 ) *
##              89) Income &gt; 100 5   0.000 Yes ( 0.00000 1.00000 ) *
##            45) CompPrice &gt; 130.5 11   0.000 Yes ( 0.00000 1.00000 ) *
##          23) Age &gt; 54.5 20  22.490 No ( 0.75000 0.25000 )  
##            46) CompPrice &lt; 122.5 10   0.000 No ( 1.00000 0.00000 ) *
##            47) CompPrice &gt; 122.5 10  13.860 No ( 0.50000 0.50000 )  
##              94) Price &lt; 125 5   0.000 Yes ( 0.00000 1.00000 ) *
##              95) Price &gt; 125 5   0.000 No ( 1.00000 0.00000 ) *
##     3) ShelveLoc: Good 85  90.330 Yes ( 0.22353 0.77647 )  
##       6) Price &lt; 135 68  49.260 Yes ( 0.11765 0.88235 )  
##        12) US: No 17  22.070 Yes ( 0.35294 0.64706 )  
##          24) Price &lt; 109 8   0.000 Yes ( 0.00000 1.00000 ) *
##          25) Price &gt; 109 9  11.460 No ( 0.66667 0.33333 ) *
##        13) US: Yes 51  16.880 Yes ( 0.03922 0.96078 ) *
##       7) Price &gt; 135 17  22.070 No ( 0.64706 0.35294 )  
##        14) Income &lt; 46 6   0.000 No ( 1.00000 0.00000 ) *
##        15) Income &gt; 46 11  15.160 Yes ( 0.45455 0.54545 ) *
</pre></div>
</div></div>

<p>
# In order to properly evalute the performance of a classification tree on these data, we must estimate the test error 
# rather than simply computing the training error. We split the observations into a training set and a test set, build
# the tree usign the training set, and evalute its performance on the test data. The predict() function can be used 
# for this purpose. In the case fo a classification tree, the agrument type = "class" instructs R to return the actual
# class prediction. This approach leads to correct predictions for around 71.5% of the locations in the test data set.
 </p>

<div class="chunk" id="unnamed-chunk-8"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">set.seed</span><span class="hl std">(</span><span class="hl num">2</span><span class="hl std">)</span>
<span class="hl kwd">nrow</span><span class="hl std">(Carseats)</span>
</pre></div>
<div class="output"><pre class="knitr r">## [1] 400
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl std">train</span> <span class="hl kwb">=</span> <span class="hl kwd">sample</span><span class="hl std">(</span><span class="hl num">1</span><span class="hl opt">:</span><span class="hl kwd">nrow</span><span class="hl std">(Carseats),</span> <span class="hl num">200</span><span class="hl std">)</span>  <span class="hl com"># take 200 rows at random from the Carseats dataset. These are out training</span>
<span class="hl std">Carseats.test</span> <span class="hl kwb">=</span> <span class="hl std">Carseats[</span><span class="hl opt">-</span><span class="hl std">train,]</span> <span class="hl com">#the non-training rows are now the test set.</span>
<span class="hl std">High.test</span> <span class="hl kwb">=</span> <span class="hl std">High[</span><span class="hl opt">-</span><span class="hl std">train]</span>  <span class="hl com"># take the rows that were above that $8K threshold, which we called &quot;High&quot;</span>
<span class="hl std">tree.carseats</span> <span class="hl kwb">=</span> <span class="hl kwd">tree</span><span class="hl std">(High</span> <span class="hl opt">~</span> <span class="hl std">.</span> <span class="hl opt">-</span><span class="hl std">Sales, Carseats,</span> <span class="hl kwc">subset</span> <span class="hl std">= train)</span> <span class="hl com">#run our model using tree() package</span>
<span class="hl std">tree.pred</span> <span class="hl kwb">=</span> <span class="hl kwd">predict</span><span class="hl std">(tree.carseats, Carseats.test,</span> <span class="hl kwc">type</span><span class="hl std">=</span><span class="hl str">&quot;class&quot;</span><span class="hl std">)</span>  <span class="hl com">#make prediction using test set</span>
<span class="hl kwd">table</span><span class="hl std">(tree.pred, High.test)</span>
</pre></div>
<div class="output"><pre class="knitr r">##          High.test
## tree.pred  No Yes
##       No  104  33
##       Yes  13  50
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl std">mytable</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">table</span><span class="hl std">(tree.pred, High.test)</span>
<span class="hl std">((mytable[</span><span class="hl str">&quot;No&quot;</span><span class="hl std">,</span> <span class="hl str">&quot;No&quot;</span><span class="hl std">]</span> <span class="hl opt">+</span> <span class="hl std">mytable[</span><span class="hl str">&quot;Yes&quot;</span><span class="hl std">,</span><span class="hl str">&quot;Yes&quot;</span><span class="hl std">] )</span><span class="hl opt">/</span> <span class="hl kwd">sum</span><span class="hl std">(mytable) )</span>
</pre></div>
<div class="output"><pre class="knitr r">## [1] 0.77
</pre></div>
</div></div>

<p># Tree Pruning
Next, we consider wheterh pruning the tree might lead to improved results. The function cv.tree() performs cross-
# validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to 
# select a sequence of trees for consideration. We use the agrument FUN=prune.misclass in order to indicate that we want
# the classification error rate to guide the cross-validation and pruning process, rather than the default for the 
# cv.tree() function, which is deviance. The cv.tree() function reports the number of termnal nodes of each tree 
# considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k,
# which corresponds to alpha in (8.4)).</p>

<div class="chunk" id="unnamed-chunk-9"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">set.seed</span><span class="hl std">(</span><span class="hl num">3</span><span class="hl std">)</span>
<span class="hl std">cv.carseats</span> <span class="hl kwb">=</span> <span class="hl kwd">cv.tree</span><span class="hl std">(tree.carseats,</span> <span class="hl kwc">FUN</span><span class="hl std">=prune.misclass)</span>
<span class="hl kwd">names</span><span class="hl std">(cv.carseats)</span>
</pre></div>
<div class="output"><pre class="knitr r">## [1] &quot;size&quot;   &quot;dev&quot;    &quot;k&quot;      &quot;method&quot;
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl std">cv.carseats</span>
</pre></div>
<div class="output"><pre class="knitr r">## $size
## [1] 21 19 14  9  8  5  3  2  1
## 
## $dev
## [1] 74 76 81 81 75 77 78 85 81
## 
## $k
## [1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0
## 
## $method
## [1] &quot;misclass&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;prune&quot;         &quot;tree.sequence&quot;
</pre></div>
</div></div>

<p># Note that, despite the name, dev corresponds to the cross-validation error rate in this instance. The tree with 9 
# terminal nodes results in teh lowest cross-validation error rate, with 50 cross validation errors. We plot the error
# rate as a function of both size and k: </p>

<div class="chunk" id="unnamed-chunk-10"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">par</span><span class="hl std">(</span><span class="hl kwc">mfrow</span><span class="hl std">=</span><span class="hl kwd">c</span><span class="hl std">(</span><span class="hl num">1</span><span class="hl std">,</span><span class="hl num">2</span><span class="hl std">))</span>
<span class="hl kwd">plot</span><span class="hl std">(cv.carseats</span><span class="hl opt">$</span><span class="hl std">size, cv.carseats</span><span class="hl opt">$</span><span class="hl std">dev,</span> <span class="hl kwc">type</span><span class="hl std">=</span><span class="hl str">&quot;b&quot;</span><span class="hl std">)</span>
<span class="hl kwd">plot</span><span class="hl std">(cv.carseats</span><span class="hl opt">$</span><span class="hl std">k ,cv.carseats</span><span class="hl opt">$</span><span class="hl std">dev,</span> <span class="hl kwc">type</span><span class="hl std">=</span><span class="hl str">&quot;b&quot;</span><span class="hl std">)</span>
</pre></div>
</div><div class="rimage default"><img src="figure/unnamed-chunk-10-1.png" title="plot of chunk unnamed-chunk-10" alt="plot of chunk unnamed-chunk-10" class="plot" /></div></div>

<p># We now apply the prune.misclass() function in order to prune the tree to obtain the nine-node tree </p>
<div class="chunk" id="unnamed-chunk-11"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">prune.carseats</span> <span class="hl kwb">=</span> <span class="hl kwd">prune.misclass</span><span class="hl std">(tree.carseats,</span> <span class="hl kwc">best</span><span class="hl std">=</span><span class="hl num">9</span><span class="hl std">)</span>
<span class="hl kwd">plot</span><span class="hl std">(prune.carseats)</span>
<span class="hl kwd">text</span><span class="hl std">(prune.carseats,</span> <span class="hl kwc">pretty</span><span class="hl std">=</span><span class="hl num">0</span><span class="hl std">)</span>
</pre></div>
</div><div class="rimage default"><img src="figure/unnamed-chunk-11-1.png" title="plot of chunk unnamed-chunk-11" alt="plot of chunk unnamed-chunk-11" class="plot" /></div></div>

<p># How well does this pruned tree perform on the test data set? Once again, we apply the predict() function. </p>
<div class="chunk" id="unnamed-chunk-12"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">tree.pred</span> <span class="hl kwb">=</span> <span class="hl kwd">predict</span><span class="hl std">(prune.carseats, Carseats.test,</span> <span class="hl kwc">type</span><span class="hl std">=</span><span class="hl str">&quot;class&quot;</span><span class="hl std">)</span>
<span class="hl kwd">table</span><span class="hl std">(tree.pred,High.test)</span>
</pre></div>
<div class="output"><pre class="knitr r">##          High.test
## tree.pred No Yes
##       No  97  25
##       Yes 20  58
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl std">mytable_pruned</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">table</span><span class="hl std">(tree.pred,High.test)</span>
<span class="hl std">((mytable_pruned[</span><span class="hl str">&quot;No&quot;</span><span class="hl std">,</span> <span class="hl str">&quot;No&quot;</span><span class="hl std">]</span> <span class="hl opt">+</span> <span class="hl std">mytable_pruned[</span><span class="hl str">&quot;Yes&quot;</span><span class="hl std">,</span><span class="hl str">&quot;Yes&quot;</span><span class="hl std">] )</span><span class="hl opt">/</span> <span class="hl kwd">sum</span><span class="hl std">(mytable_pruned) )</span>
</pre></div>
<div class="output"><pre class="knitr r">## [1] 0.775
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl com"># from book: (94+60)/200</span>
</pre></div>
</div></div>

<p># Now 77.5% of the test observatiosn are correctly classified, so not only has the pruning process produced a more
# interpreterable tree, but it has also improved the classification accuracy.
# If we increases the value of best, we obtain a larger pruned tree with lower classification accuracy: </p>
<div class="chunk" id="unnamed-chunk-13"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">prune.carseats</span> <span class="hl kwb">=</span> <span class="hl kwd">prune.misclass</span><span class="hl std">(tree.carseats,</span> <span class="hl kwc">best</span><span class="hl std">=</span><span class="hl num">15</span><span class="hl std">)</span>  <span class="hl com"># increased from 9</span>
<span class="hl kwd">plot</span><span class="hl std">(prune.carseats)</span>
<span class="hl kwd">text</span><span class="hl std">(prune.carseats,</span> <span class="hl kwc">pretty</span><span class="hl std">=</span><span class="hl num">0</span><span class="hl std">)</span>
</pre></div>
</div><div class="rimage default"><img src="figure/unnamed-chunk-13-1.png" title="plot of chunk unnamed-chunk-13" alt="plot of chunk unnamed-chunk-13" class="plot" /></div><div class="rcode">
<div class="source"><pre class="knitr r"><span class="hl std">tree.pred</span> <span class="hl kwb">=</span> <span class="hl kwd">predict</span><span class="hl std">(prune.carseats,Carseats.test,</span><span class="hl kwc">type</span><span class="hl std">=</span><span class="hl str">&quot;class&quot;</span><span class="hl std">)</span>
<span class="hl kwd">table</span><span class="hl std">(tree.pred,High.test)</span>
</pre></div>
<div class="output"><pre class="knitr r">##          High.test
## tree.pred  No Yes
##       No  102  30
##       Yes  15  53
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl std">mytable_pruned15</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">table</span><span class="hl std">(tree.pred,High.test)</span>
<span class="hl std">((mytable_pruned15[</span><span class="hl str">&quot;No&quot;</span><span class="hl std">,</span> <span class="hl str">&quot;No&quot;</span><span class="hl std">]</span> <span class="hl opt">+</span> <span class="hl std">mytable_pruned15[</span><span class="hl str">&quot;Yes&quot;</span><span class="hl std">,</span><span class="hl str">&quot;Yes&quot;</span><span class="hl std">] )</span><span class="hl opt">/</span> <span class="hl kwd">sum</span><span class="hl std">(mytable_pruned15) )</span>
</pre></div>
<div class="output"><pre class="knitr r">## [1] 0.775
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl com"># exact same classifcaiton performance as with best = 9</span>
</pre></div>
</div></div>

</body>
</html>
